# -*- coding: utf-8 -*-
"""Dysarthria_whisper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XfovWu-eFNpkkUDbx_HlUCgNfAPw02O0
"""

import shutil

shutil.rmtree('/content/dataset')

import os
import zipfile  # zipfile 모듈을 가져옵니다.

# 압축 파일 경로 (업로드한 파일의 경로에 따라 변경)
zip_file_path = '/content/archive.zip'
output_dir = '/content/dataset'

# 압축 해제
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(output_dir)

print("압축 해제가 완료되었습니다.")

# 데이터셋 구조 확인
for root, dirs, files in os.walk(output_dir):
    print(root, len(files), "개의 파일")

"""### Setting

❗️This is setting. Run all this
"""

!pip install transformers
!pip install --upgrade transformers

!pip install -q datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate
!pip install -q git+https://github.com/huggingface/peft.git@main
!pip install torchaudio
!pip install --upgrade bitsandbytes
!pip install --upgrade peft

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""### Load Dataset

❗️Upload train and validation .pt file, and run below codes.
"""

import torch

train_data = torch.load("train_data_onlyDY.pt")
val_data = torch.load("val_data_onlyDY.pt")

# 데이터셋 크기 확인
print(f"Train 데이터셋 크기: {len(train_data)}")
print(f"Validation 데이터셋 크기: {len(val_data)}")

from datasets import Dataset, DatasetDict


# 여러 스플릿으로 관리하기 위한 DatasetDict 생성
dataset_dict = DatasetDict({
    'train': Dataset.from_list(train_data),   # train split
    'validation': Dataset.from_list(val_data),  # validation split
})

from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor

model_name = "openai/whisper-small"
feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)
tokenizer = WhisperTokenizer.from_pretrained(model_name, language="en", task="transcribe")
processor = WhisperProcessor.from_pretrained(model_name, language="en", task="transcribe")

def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    # encode target text to label ids
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch

data = dataset_dict.map(prepare_dataset, remove_columns=dataset_dict.column_names["train"])

data

"""### Define a datacollactor"""

import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need different padding methods
        # first treat the audio inputs by simply returning torch tensors
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # get the tokenized label sequences
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # pad the labels to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

"""### Define metric"""

import evaluate

metric = evaluate.load("wer")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./logs

"""### Fine-tuning

❗️This is Lora parameter.

Can change: r, lora_alpha, target_modules(q,v or q,k), lora_dropout
"""

from transformers import AutoTokenizer, WhisperForConditionalGeneration
from peft import LoraConfig, get_peft_model

model_name = "openai/whisper-small"
model = WhisperForConditionalGeneration.from_pretrained(model_name, device_map="auto")

# LoRA 구성 설정
config = LoraConfig(
    r=8,
    lora_alpha=8,
    target_modules=[
        "q_proj",
        "v_proj",
    ],
    lora_dropout=0.05,
    bias="none",
    # task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, config)

"""❗️please record trainable params, all params, trainable %.

e.g.

trainable params: 884,736 || all params: 242,619,648 || trainable%: 0.3647

"""

model.print_trainable_parameters()

from torch.utils.tensorboard import SummaryWriter
import os
from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR

# TensorBoard Writer 생성
writer = SummaryWriter(log_dir="./logs")

class TensorBoardCombinedLossCallback(TrainerCallback):
    def __init__(self):
        self.train_losses = []
        self.validation_losses = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        # Training Loss 기록
        if "loss" in logs:
            self.train_losses.append((state.global_step, logs["loss"]))
            writer.add_scalars(
                "Loss/Combined",
                {"Train": logs["loss"]},
                state.global_step,
            )

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        # Validation Loss 기록
        if metrics and "eval_loss" in metrics:
            self.validation_losses.append((state.global_step, metrics["eval_loss"]))
            writer.add_scalars(
                "Loss/Combined",
                {"Validation": metrics["eval_loss"]},
                state.global_step,
            )

    def on_train_end(self, args, state, control, **kwargs):
        # 학습 종료 시 TensorBoard Writer 닫기
        writer.close()

class LogValidationLossCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        # metrics에 validation loss 포함됨
        validation_loss = metrics.get("eval_loss", None)
        if validation_loss is not None:
            state.log_history.append({"step": state.global_step, "validation_loss": validation_loss})

"""❗️Below is training parameters.

Can change: per_device_train_batch_size, gradient_accumulation_steps, learning_rate, warmup_steps, num_train_epochs, lr_scheduler_type, eval_steps, save_steps, logging_steps
"""

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="temp",  # change to a repo name of your choice
    logging_dir="./logs",       # 로그 저장 경로
    report_to="tensorboard",    # TensorBoard로 기록
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,  # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-3,
    warmup_steps=500,
    # warmup_ratio=0.1,
    # weight_decay=0.005,
    num_train_epochs=3,
    evaluation_strategy="steps",
    lr_scheduler_type="cosine",
    eval_steps=25,
    save_steps=25,
    fp16=True,
    generation_max_length=128,
    logging_steps=25,
    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward
    label_names=["labels"],  # same reason as above
)

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=data["train"],
    eval_dataset=data["validation"],
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    callbacks=[LogValidationLossCallback, TensorBoardCombinedLossCallback],  # TensorBoardLossCallback 추가
)

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!

trainer.train()

"""### Evaluation"""

import torch

test_dysarthria_data = torch.load("test_dysarthria.pt")
test_nondysarthria_data = torch.load("test_non_dysarthria.pt")

from datasets import Dataset, DatasetDict

# 여러 스플릿으로 관리하기 위한 DatasetDict 생성
dataset_dict = DatasetDict({
    'dysarthria': Dataset.from_list(test_dysarthria_data),   # train split
    'non_dysarthria': Dataset.from_list(test_nondysarthria_data),  # validation split
})

dataset_dict

def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    # encode target text to label ids
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch

data = dataset_dict.map(prepare_dataset, remove_columns=dataset_dict.column_names["dysarthria"])

"""Dysarthria test set에 대한 WER"""

import gc
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers.models.whisper.english_normalizer import BasicTextNormalizer

# 커스텀 WER 계산 함수
def custom_wer(reference, hypothesis):
    """
    긴 예측 오류 또는 빈 예측 오류를 WER=1로 설정.
    """
    if not hypothesis or len(hypothesis.split()) > 2 * len(reference.split()):
        return 1.0  # WER = 1로 처리
    else:
        return metric.compute(predictions=[hypothesis], references=[reference])

# 데이터 로더 및 설정
dysarthria_data = DataLoader(data["dysarthria"], batch_size=8, collate_fn=data_collator)
forced_decoder_ids = processor.get_decoder_prompt_ids(language="en", task="transcribe")
normalizer = BasicTextNormalizer()

# 평가 결과 저장 변수
predictions = []
references = []
normalized_predictions = []
normalized_references = []

model.eval()
for step, batch in enumerate(tqdm(dysarthria_data)):
    with torch.cuda.amp.autocast():
        with torch.no_grad():
            # 예측 생성
            generated_tokens = (
                model.generate(
                    input_features=batch["input_features"].to("cuda"),
                    forced_decoder_ids=forced_decoder_ids,
                    max_new_tokens=255,
                )
                .cpu()
                .numpy()
            )
            # 라벨 처리
            labels = batch["labels"].cpu().numpy()
            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)
            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

            # 결과 저장
            predictions.extend(decoded_preds)
            references.extend(decoded_labels)
            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])
            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])

        # 메모리 정리
        del generated_tokens, labels, batch
    gc.collect()

# 커스텀 WER 계산
wer_values = [custom_wer(ref, pred) for ref, pred in zip(references, predictions)]
normalized_wer_values = [custom_wer(ref, pred) for ref, pred in zip(normalized_references, normalized_predictions)]

# 평균 WER 계산
wer = 100 * np.mean(wer_values)
normalized_wer = 100 * np.mean(normalized_wer_values)

eval_metrics = {"eval/wer": wer, "eval/normalized_wer": normalized_wer}

# 결과 출력
print(f"{wer=} and {normalized_wer=}")
print(eval_metrics)

# 보조 지표 계산
from jiwer import wer as jiwer_wer, cer as jiwer_cer
jiwer_score = jiwer_wer(references, predictions) * 100
cer_score = jiwer_cer(references, predictions) * 100

print(f"JIWER WER: {jiwer_score:.2f}%, JIWER CER: {cer_score:.2f}%")

"""❗️❗️아래 non-dysarthria에 대한 WER 계산코드는 최적 파라미터에 대해서만 진행"""

# import gc
# import numpy as np
# from tqdm import tqdm
# from torch.utils.data import DataLoader
# from transformers.models.whisper.english_normalizer import BasicTextNormalizer

# # 커스텀 WER 계산 함수
# def custom_wer(reference, hypothesis):
#     """
#     긴 예측 오류 또는 빈 예측 오류를 WER=1로 설정.
#     """
#     if not hypothesis or len(hypothesis.split()) > 2 * len(reference.split()):
#         return 1.0  # WER = 1로 처리
#     else:
#         return metric.compute(predictions=[hypothesis], references=[reference])

# # 데이터 로더 및 설정
# non_dysarthria_data = DataLoader(test["non_dysarthria"], batch_size=8, collate_fn=data_collator)
# forced_decoder_ids = processor.get_decoder_prompt_ids(language="en", task="transcribe")
# normalizer = BasicTextNormalizer()

# # 평가 결과 저장 변수
# predictions = []
# references = []
# normalized_predictions = []
# normalized_references = []

# model.eval()
# for step, batch in enumerate(tqdm(non_dysarthria_data)):
#     with torch.cuda.amp.autocast():
#         with torch.no_grad():
#             # 예측 생성
#             generated_tokens = (
#                 model.generate(
#                     input_features=batch["input_features"].to("cuda"),
#                     forced_decoder_ids=forced_decoder_ids,
#                     max_new_tokens=255,
#                 )
#                 .cpu()
#                 .numpy()
#             )
#             # 라벨 처리
#             labels = batch["labels"].cpu().numpy()
#             labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)
#             decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
#             decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

#             # 결과 저장
#             predictions.extend(decoded_preds)
#             references.extend(decoded_labels)
#             normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])
#             normalized_references.extend([normalizer(label).strip() for label in decoded_labels])

#         # 메모리 정리
#         del generated_tokens, labels, batch
#     gc.collect()

# # 커스텀 WER 계산
# wer_values = [custom_wer(ref, pred) for ref, pred in zip(references, predictions)]
# normalized_wer_values = [custom_wer(ref, pred) for ref, pred in zip(normalized_references, normalized_predictions)]

# # 평균 WER 계산
# wer = 100 * np.mean(wer_values)
# normalized_wer = 100 * np.mean(normalized_wer_values)

# eval_metrics = {"eval/wer": wer, "eval/normalized_wer": normalized_wer}

# # 결과 출력
# print(f"{wer=} and {normalized_wer=}")
# print(eval_metrics)

# 보조 지표 계산
# from jiwer import wer as jiwer_wer, cer as jiwer_cer
# jiwer_score = jiwer_wer(references, predictions) * 100
# cer_score = jiwer_cer(references, predictions) * 100

# print(f"JIWER WER: {jiwer_score:.2f}%, JIWER CER: {cer_score:.2f}%")

"""❗️아래 코드 실행x"""

# for idx, (pred, ref) in enumerate(zip(predictions, references)):
#     print(f"Index {idx}:\nPrediction: {pred}\nReference: {ref}\n")

# def compare_lists(predictions, references):
#     # Initialize counters
#     exact_matches = 0
#     mismatches = 0

#     for pred, ref in zip(predictions, references):
#         # Remove spaces for comparison
#         pred_clean = pred.replace(" ", "")
#         ref_clean = ref.replace(" ", "")

#         if pred_clean == ref_clean:
#             exact_matches += 1
#         else:
#             mismatches += 1

#     return exact_matches, mismatches


# # Compare the lists and get results
# exact_matches, mismatches = compare_lists(predictions, references)

# # Print the results
# print(f"Exact Matches: {exact_matches}")
# print(f"Mismatches: {mismatches}")