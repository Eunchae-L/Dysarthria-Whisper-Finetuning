# -*- coding: utf-8 -*-
"""WhisperS2T_TensorRT_LLM (librispeech test-clean dataset).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_HDNUbW37E0nH6ShJJ8hn86y76bvwKz

<center>
<h1>WhisperS2T Demo TensorRT-LLM Backend</h1>
<a target="_blank" href="https://colab.research.google.com/github/shashikg/WhisperS2T/blob/main/notebooks/WhisperS2T_TensorRT_LLM.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>
</center>

## Environment Preparation
"""

!pip install ipython_autotime

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autotime

!pip uninstall -y torch torchvision torchaudio
!pip cache purge
!pip install torch==2.1.2+cu121 torchvision torchaudio==2.1.2+cu121 -f https://download.pytorch.org/whl/torch_stable.html

import torch
import torchvision
import torchaudio

print("Torch version:", torch.__version__)
print("Torchvision version:", torchvision.__version__)
print("Torchaudio version:", torchaudio.__version__)
print("CUDA available:", torch.cuda.is_available())

!apt-get update && apt-get install -y libsndfile1 ffmpeg
!pip install -U git+https://github.com/shashikg/WhisperS2T.git

!wget https://github.com/shashikg/WhisperS2T/raw/main/install_tensorrt.sh
!bash install_tensorrt.sh

!wget https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/sam_altman_lex_podcast_367.flac

"""## Load Model"""

import whisper_s2t
model = whisper_s2t.load_model(model_identifier="large-v2", backend='TensorRT-LLM')

# Note: For first run the model may give slightly slower inference speed. After 1-2 runs it gives better inference speed.
# This is due to the JIT tracing of the VAD model and TensorRT Engine -- they need some warmups before running actual inference.

files = ['sam_altman_lex_podcast_367.flac']
lang_codes = ['en']
tasks = ['transcribe']
initial_prompts = [None]

out = model.transcribe_with_vad(files,
                                lang_codes=lang_codes,
                                tasks=tasks,
                                initial_prompts=initial_prompts,
                                batch_size=24)

print(out[0][0])

"""## Downloading the Librispeech (test-clean) dataset."""

import requests

# URL for the test-clean dataset
url = "http://www.openslr.org/resources/12/test-clean.tar.gz"
output_path = "test-clean.tar.gz"

# Download the file
print("Downloading test-clean dataset...")
response = requests.get(url, stream=True)
with open(output_path, "wb") as file:
    for chunk in response.iter_content(chunk_size=8192):
        if chunk:
            file.write(chunk)
print("Download complete.")

import tarfile

# Extract the downloaded tar.gz file
print("Extracting test-clean dataset...")
with tarfile.open("test-clean.tar.gz", "r:gz") as tar:
    tar.extractall(path="./test-clean/")
print("Extraction complete.")

import os

# Check the extracted files
dataset_path = "./test-clean/LibriSpeech/test-clean"
for root, dirs, files in os.walk(dataset_path):
    print(f"Directory: {root}")
    for file in files:
        print(f"  File: {file}")

import os

# Path to the LibriSpeech test-clean dataset
dataset_path = "/content/test-clean"

# Collect all .flac files
audio_files = []
for root, _, files in os.walk(dataset_path):
    for file in files:
        if file.endswith(".flac"):
            audio_files.append(os.path.join(root, file))

print(f"Found {len(audio_files)} audio files in test-clean.")

# Prepare parameters for transcription
lang_codes = ['en'] * len(audio_files)       # Assuming all audio is in English
tasks = ['transcribe'] * len(audio_files)   # All tasks are transcription
initial_prompts = [None] * len(audio_files) # No initial prompts

# Set batch size for processing
batch_size = 24
results = []

# Process files in batches
for i in range(0, len(audio_files), batch_size):
    batch_files = audio_files[i:i+batch_size]
    batch_lang_codes = lang_codes[i:i+batch_size]
    batch_tasks = tasks[i:i+batch_size]
    batch_initial_prompts = initial_prompts[i:i+batch_size]

    # Transcribe the current batch
    out = model.transcribe_with_vad(
        batch_files,
        lang_codes=batch_lang_codes,
        tasks=batch_tasks,
        initial_prompts=batch_initial_prompts,
        batch_size=batch_size
    )
    results.extend(out)

print(f"Transcription completed for {len(audio_files)} audio files.")

# Save results with UTF-8 encoding
with open("test_clean_transcriptions_with_metadata.txt", "w", encoding="utf-8") as f:
    for audio_file, transcription in zip(audio_files, results):
        f.write(f"{audio_file}:\n")
        for segment in transcription:  # Iterate through the list of dictionaries
            f.write(f"  Text: {segment['text']}\n")
            f.write(f"  Start Time: {segment['start_time']}\n")
            f.write(f"  End Time: {segment['end_time']}\n")
        f.write("\n")  # Add a blank line between entries

print("Transcriptions with metadata saved to test_clean_transcriptions_with_metadata.txt.")

"""## Evaluation: Calculate WER"""

import subprocess

# Install jiwer using subprocess
subprocess.run(["pip", "install", "jiwer"])

import jiwer

print("jiwer installed successfully!")

# Path to the LibriSpeech ground truth files
ground_truth_path = "./test-clean/LibriSpeech/test-clean"

# Dictionary to store ground truth transcriptions
ground_truths = {}

# Read ground truth transcriptions
for root, _, files in os.walk(ground_truth_path):
    for file in files:
        if file.endswith(".txt"):  # Find .txt files with transcriptions
            with open(os.path.join(root, file), "r") as f:
                for line in f:
                    parts = line.strip().split(" ", 1)
                    if len(parts) == 2:
                        audio_id, text = parts
                        ground_truths[audio_id] = text.upper()  # LibriSpeech uses uppercase text

# Extract audio IDs from file paths
predictions = {}
for audio_file, transcription in zip(audio_files, results):
    audio_id = os.path.basename(audio_file).replace(".flac", "")  # Extract ID
    predicted_text = " ".join([segment["text"] for segment in transcription])  # Combine segments
    predictions[audio_id] = predicted_text.upper()  # Convert to uppercase for comparison

from jiwer import wer

# Calculate WER for all files
wer_scores = {}
for audio_id in predictions.keys():
    if audio_id in ground_truths:
        wer_scores[audio_id] = wer(ground_truths[audio_id], predictions[audio_id])
    else:
        print(f"Warning: Missing ground truth for {audio_id}")

# Average WER
average_wer = sum(wer_scores.values()) / len(wer_scores)
print(f"Average Word Error Rate (WER): {average_wer:.2%}")

# Save WER results to a file
with open("wer_results.txt", "w", encoding="utf-8") as f:
    for audio_id, score in wer_scores.items():
        f.write(f"{audio_id}: {score:.2%}\n")

print("WER results saved to wer_results.txt.")